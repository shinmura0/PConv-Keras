{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ano-Unet_V2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xHabEBOYUSPC","colab_type":"text"},"source":["#data load"]},{"cell_type":"code","metadata":{"id":"P1pvbbTg55KN","colab_type":"code","outputId":"518b4b4c-10a5-402c-82ec-11534dab2fde","executionInfo":{"status":"ok","timestamp":1570152134159,"user_tz":-540,"elapsed":2440,"user":{"displayName":"Rubu Kuma","photoUrl":"","userId":"02985854947085672136"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from PIL import Image\n","import sys, os, urllib.request, tarfile, cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.utils import to_categorical\n","from keras.preprocessing import image\n","\n","def convert(x, size=96):\n","    result = []\n","    for i in range(len(x)):\n","        img = cv2.resize(x[i],(size, size))\n","        result.append(img)\n","        \n","    return np.array(result)\n","\n","class AD:\n","    def __init__(self, download_dir, path):\n","        self.path = \"data/\"\n","\n","        if not os.path.exists(download_dir):\n","            os.mkdir(download_dir)\n","\n","        # download file\n","        def _progress(count, block_size, total_size):\n","            sys.stdout.write('\\rDownloading %s %.2f%%' % (source_path,\n","                float(count * block_size) / float(total_size) * 100.0))\n","            sys.stdout.flush()\n","\n","        source_path = path\n","        dest_path = os.path.join(download_dir, \"data.tar.xz\")\n","        urllib.request.urlretrieve(source_path, filename=dest_path, reporthook=_progress)\n","        # untar\n","        with tarfile.open(dest_path, \"r:xz\") as tar:\n","            tar.extractall(self.path)\n","\n","    def load_images(self, path, num):\n","        result = []\n","        for i in range(num):\n","            if i < 10:\n","                img = Image.open(self.path + path + \"00\" + str(i) + \".png\")\n","            elif i < 100:\n","                img = Image.open(self.path + path + \"0\" + str(i) + \".png\")\n","            else:\n","                img = Image.open(self.path + path + str(i) + \".png\")\n","            img = image.img_to_array(img)\n","            img = cv2.resize(img,(224,224))\n","            result.append(img)\n","        return np.array(result)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LMNxoy2BydlD","colab_type":"code","outputId":"44a23890-0340-45ca-c544-1b30dcc4cbda","executionInfo":{"status":"ok","timestamp":1570152248933,"user_tz":-540,"elapsed":117190,"user":{"displayName":"Rubu Kuma","photoUrl":"","userId":"02985854947085672136"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(\"\\nHazelnut data download...\")\n","Hazelnut = AD(\"./ad\", \"ftp://guest:GU.205dldo@ftp.softronics.ch/mvtec_anomaly_detection/hazelnut.tar.xz\")\n","hazelnut_train = Hazelnut.load_images(\"hazelnut/train/good/\", 391)\n","hazelnut_test_normal = Hazelnut.load_images(\"hazelnut/test/good/\", 40)\n","hazelnut_test_anomaly = Hazelnut.load_images(\"hazelnut/test/crack/\", 18)\n","hazelnut_test_anomaly = np.vstack((hazelnut_test_anomaly, Hazelnut.load_images(\"hazelnut/test/cut/\", 17)))\n","hazelnut_test_anomaly = np.vstack((hazelnut_test_anomaly, Hazelnut.load_images(\"hazelnut/test/print/\", 17)))\n","hazelnut_test_anomaly = np.vstack((hazelnut_test_anomaly, Hazelnut.load_images(\"hazelnut/test/hole/\", 18)))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\n","Hazelnut data download...\n","Downloading ftp://guest:GU.205dldo@ftp.softronics.ch/mvtec_anomaly_detection/hazelnut.tar.xz 100.00%"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lHwSQ9RFEhQN","colab_type":"code","colab":{}},"source":["hazelnut_train /= 255\n","hazelnut_test_normal /= 255\n","hazelnut_test_anomaly /= 255\n","\n","x_train = convert(hazelnut_train)\n","x_test_normal = convert(hazelnut_test_normal)\n","x_test_anomaly = convert(hazelnut_test_anomaly)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s85igwXtVp37","colab_type":"text"},"source":["#異常検知の学習"]},{"cell_type":"markdown","metadata":{"id":"M2Q3g5_9pslS","colab_type":"text"},"source":["##cifar10"]},{"cell_type":"code","metadata":{"id":"oncS6Sz2J4Rt","colab_type":"code","colab":{}},"source":["import keras\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.image import array_to_img, img_to_array\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from keras.datasets import cifar10\n","from keras.utils import to_categorical\n","from keras.layers import Input, Dense, Activation\n","from keras.optimizers import SGD, Adam\n","from keras.models import Model\n","from keras import backend as K\n","from keras.applications import MobileNetV2\n","\n","Height = 96\n","Width = 96\n","channel = 3\n","\n","def resize(x):\n","    x_out = []\n","    \n","    for i in range(len(x)):\n","        img = cv2.resize(x[i], dsize=(Width, Height))\n","        x_out.append(img)\n","                \n","    return np.array(x_out)\n","\n","def cifar(x_class6):\n","    # dataset\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","    x_train = x_train.astype('float32') / 255\n","    x_test = x_test.astype('float32') / 255\n","\n","    #refデータからランダムに6000個抽出\n","    number = np.random.choice(np.arange(0,len(x_train)),3000,replace=False)\n","\n","    x, y = [], []\n","\n","    for i in number:\n","        x.append(x_train[i])\n","        y.append(y_train[i])\n","    \n","    x = resize(x)\n","\n","    #正常データからランダムに300個抽出\n","    number = np.random.choice(np.arange(0,len(x_class6)),300,replace=False)\n","\n","    xx, yy = [], []\n","\n","    for i in number:\n","        xx.append(x_class6[i])\n","        yy.append(10)\n","    \n","    #正常データと結合\n","    x = np.vstack((x,xx))\n","    y = np.vstack((y,np.array(yy).reshape((-1,1))))\n","\n","    X_train = np.array(x)\n","    Y_train = to_categorical(y)\n","\n","    print(X_train.shape)\n","    print(Y_train.shape)\n","    print(x_test_anomaly.shape)\n","    print(np.max(X_train))\n","    print(np.min(X_train))\n","    print(np.max(x_test_anomaly))\n","    print(np.min(x_test_anomaly))\n","\n","    return X_train, Y_train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ya2zXbsVzH7_","colab_type":"code","outputId":"9d7ab741-06ed-4647-9581-333cf90884be","executionInfo":{"status":"ok","timestamp":1570152254640,"user_tz":-540,"elapsed":118229,"user":{"displayName":"Rubu Kuma","photoUrl":"","userId":"02985854947085672136"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["X_train, Y_train = cifar(x_train)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n","(3300, 96, 96, 3)\n","(3300, 11)\n","(70, 96, 96, 3)\n","1.0\n","0.0\n","1.0\n","0.08235294\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IYns12g3BCsI","colab_type":"text"},"source":["##MobileNet V2"]},{"cell_type":"code","metadata":{"id":"W8ICGE05lasU","colab_type":"code","colab":{}},"source":["def mobilenet(x, y):\n","    classes = 11\n","    #alpha_ = 5\n","\n","    mobile = MobileNetV2(include_top=True, input_shape=x.shape[1:], alpha=1.0,\n","                         weights='imagenet')\n","    \n","    # 最終層削除\n","    f_model = Model(inputs=mobile.input,outputs=mobile.layers[-2].output)\n","            \n","    # (L2層と)全結合層を付ける\n","    #c = keras.layers.Lambda(lambda xx: alpha_*(xx)/K.sqrt(K.sum(xx**2)))(f_model.output) #metric learning\n","    c = Dense(classes, activation='softmax')(f_model.output)\n","    f_model = Model(inputs=f_model.input,outputs=c)\n","\n","    f_model.compile(loss='categorical_crossentropy',\n","                  optimizer = SGD(lr=5e-4, decay=0.00005),#Adam(lr=0.0001, amsgrad=True)\n","                  metrics=['accuracy'])\n","    \n","    hist = f_model.fit(x,\n","                       y,\n","                       batch_size=64,\n","                       shuffle = True,\n","                       epochs=50,#30,\n","                       verbose = True)\n","\n","    return f_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-mWktqCzjrM","colab_type":"code","outputId":"9e19d92e-e2c9-43e7-db9c-41449fb23b4c","executionInfo":{"status":"ok","timestamp":1570152719263,"user_tz":-540,"elapsed":445929,"user":{"displayName":"Rubu Kuma","photoUrl":"","userId":"02985854947085672136"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = mobilenet(X_train, Y_train)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96.h5\n","14540800/14536120 [==============================] - 0s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Epoch 1/50\n","3300/3300 [==============================] - 17s 5ms/step - loss: 2.6700 - acc: 0.1703\n","Epoch 2/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 2.0121 - acc: 0.3167\n","Epoch 3/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 1.6471 - acc: 0.4506\n","Epoch 4/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 1.3508 - acc: 0.5694\n","Epoch 5/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 1.1521 - acc: 0.6530\n","Epoch 6/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.9931 - acc: 0.7012\n","Epoch 7/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.8934 - acc: 0.7421\n","Epoch 8/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.7863 - acc: 0.7782\n","Epoch 9/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.7121 - acc: 0.7988\n","Epoch 10/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.6543 - acc: 0.8182\n","Epoch 11/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.5901 - acc: 0.8361\n","Epoch 12/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.5493 - acc: 0.8524\n","Epoch 13/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.5140 - acc: 0.8606\n","Epoch 14/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.4722 - acc: 0.8697\n","Epoch 15/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.4403 - acc: 0.8812\n","Epoch 16/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.4211 - acc: 0.8864\n","Epoch 17/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.3945 - acc: 0.8967\n","Epoch 18/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.3780 - acc: 0.9030\n","Epoch 19/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.3704 - acc: 0.8997\n","Epoch 20/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.3439 - acc: 0.9094\n","Epoch 21/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.3202 - acc: 0.9194\n","Epoch 22/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.3066 - acc: 0.9267\n","Epoch 23/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2896 - acc: 0.9288\n","Epoch 24/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2797 - acc: 0.9330\n","Epoch 25/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2675 - acc: 0.9345\n","Epoch 26/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2534 - acc: 0.9394\n","Epoch 27/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2489 - acc: 0.9391\n","Epoch 28/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2355 - acc: 0.9515\n","Epoch 29/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2311 - acc: 0.9491\n","Epoch 30/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2117 - acc: 0.9555\n","Epoch 31/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.2019 - acc: 0.9588\n","Epoch 32/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1916 - acc: 0.9588\n","Epoch 33/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1916 - acc: 0.9600\n","Epoch 34/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1900 - acc: 0.9597\n","Epoch 35/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1724 - acc: 0.9688\n","Epoch 36/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1780 - acc: 0.9630\n","Epoch 37/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1651 - acc: 0.9676\n","Epoch 38/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1603 - acc: 0.9697\n","Epoch 39/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1462 - acc: 0.9755\n","Epoch 40/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1533 - acc: 0.9730\n","Epoch 41/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1403 - acc: 0.9764\n","Epoch 42/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1402 - acc: 0.9758\n","Epoch 43/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1371 - acc: 0.9776\n","Epoch 44/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1324 - acc: 0.9785\n","Epoch 45/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1293 - acc: 0.9806\n","Epoch 46/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1201 - acc: 0.9830\n","Epoch 47/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1186 - acc: 0.9833\n","Epoch 48/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1185 - acc: 0.9815\n","Epoch 49/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1130 - acc: 0.9839\n","Epoch 50/50\n","3300/3300 [==============================] - 8s 3ms/step - loss: 0.1112 - acc: 0.9839\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mroW_wwIlk7n","colab_type":"code","colab":{}},"source":["# 層削除\n","f_model = Model(inputs=model.input,outputs=model.layers[-24].output, name=\"f_net\")\n","#f_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FuRJBBO8sahu","colab_type":"text"},"source":["#Partial Convolutions(Unet)"]},{"cell_type":"code","metadata":{"id":"KwTRinuo4_R2","colab_type":"code","outputId":"f405243d-c99e-4a0f-ded2-fd6a76831caa","executionInfo":{"status":"ok","timestamp":1569480638601,"user_tz":-540,"elapsed":5393,"user":{"displayName":"Rubu Kuma","photoUrl":"","userId":"02985854947085672136"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["#Partial Convolutionsの最小サイズは256*256のため画像をリサイズ\n","x_class6_256 = convert(hazelnut_train, size=256)\n","x_test_anomaly_256 = convert(hazelnut_test_anomaly, size=256)\n","\n","!git clone https://github.com/shinmura0/PConv-Keras\n","%cd PConv-Keras/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'PConv-Keras'...\n","remote: Enumerating objects: 20, done.\u001b[K\n","remote: Counting objects: 100% (20/20), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 445 (delta 9), reused 0 (delta 0), pack-reused 425\n","Receiving objects: 100% (445/445), 63.89 MiB | 44.02 MiB/s, done.\n","Resolving deltas: 100% (121/121), done.\n","/content/PConv-Keras\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AHP_2YrG5C29","colab_type":"code","colab":{}},"source":["import gc\n","from copy import deepcopy\n","import cv2\n","import numpy as np\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from libs.util import random_mask\n","from libs.pconv_model import PConvUnet\n","\n","# Settings\n","BATCH_SIZE = 4\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","class DataGenerator(ImageDataGenerator):\n","    def flow(self, x, *args, **kwargs):\n","        while True:\n","            \n","            # Get augmentend image samples\n","            ori = next(super().flow(x, *args, **kwargs))\n","\n","            # Get masks for each image sample\n","            mask = np.stack([random_mask(ori.shape[1], ori.shape[2]) for _ in range(ori.shape[0])], axis=0)\n","\n","            # Apply masks to all image sample\n","            masked = deepcopy(ori)\n","            masked[mask==0] = 1\n","\n","            # Yield ([ori, masl],  ori) training batches\n","            # print(masked.shape, ori.shape)\n","            gc.collect()\n","            yield [masked, mask], ori        \n","\n","# Create datagen\n","train_datagen = DataGenerator(rotation_range=10,\n","                              #width_shift_range=0.2,\n","                              #height_shift_range=0.2,\n","                              zoom_range = [0.9,1],\n","                              horizontal_flip=True\n","                              )\n","\n","# Create generator from numpy arrays\n","train_generator = train_datagen.flow(x=x_class6_256, batch_size=BATCH_SIZE)\n","\n","# Create datagen\n","test_datagen = DataGenerator(horizontal_flip=True)\n","\n","# Get samples & Display them\n","test_generator = test_datagen.flow(x=x_test_anomaly_256, batch_size=3)\n","(masked, mask), ori = next(test_generator)\n","\n","def plot_callback(model):\n","    \"\"\"Called at the end of each epoch, displaying our previous test images,\n","    as well as their masked predictions and saving them to disk\"\"\"\n","    \n","    # Get samples & Display them        \n","    pred_img = model.predict([masked, mask])\n","\n","    # Clear current output and display test images\n","    for i in range(len(ori)):\n","        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n","        axes[0].imshow(masked[i,:,:,:])\n","        axes[1].imshow(pred_img[i,:,:,:] * 1.)\n","        axes[2].imshow(ori[i,:,:,:])\n","        axes[0].set_title('Masked Image')\n","        axes[1].set_title('Predicted Image')\n","        axes[2].set_title('Original Image')                \n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Q4IAMzFWBEp","colab_type":"text"},"source":["##学習（5,6時間かかる）"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jW8gHnDdXNa8","colab":{}},"source":["p_model = PConvUnet(img_rows=256, img_cols=256)\n","p_model.fit(train_generator,\n","            steps_per_epoch=1000,\n","            epochs=30,\n","            plot_callback=plot_callback)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWE2RfN8Wmge","colab_type":"text"},"source":["#Ano-Unet V2実行"]},{"cell_type":"code","metadata":{"id":"Rcd7kWhqBRRp","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.neighbors import LocalOutlierFactor\n","\n","def grid_score(target_img, lof, ms, p_model, f_model, divide):\n","    shape = target_img.shape\n","    result = np.zeros((shape[0], shape[1]),np.float32)\n","\n","    for i in range(divide):\n","        for j in range(divide):\n","            img = np.zeros(target_img.shape, np.uint8)\n","            x_begin = int(i*shape[1]/divide)\n","            x_end = int((i+1)*shape[0]/divide)\n","            y_begin = int(j*shape[1]/divide)\n","            y_end = int((j+1)*shape[0]/divide)\n","            cv2.rectangle(img, (x_begin, y_begin), (x_end, y_end), (1, 1, 1), thickness=-1)\n","            mask = 1-img\n","\n","            # Image + mask\n","            masked_img = deepcopy(target_img)\n","            masked_img[mask==0] = 1\n","            predict_img = p_model.predict([np.expand_dims(masked_img, axis=0), np.expand_dims(mask, axis=0)])\n","            predict_img = cv2.resize(predict_img[0], (96,96))\n","\n","            score = f_model.predict(np.expand_dims(predict_img,axis=0))\n","            score = ms.transform(score.reshape((1,-1)))\n","            score = -lof._decision_function(score)\n","\n","            result[y_begin:y_end, x_begin:x_end] = score[0]\n","\n","    result = (result - np.min(result))/(np.max(result)-np.min(result))# 0-1\n","    result = 1-result # 異常度大きい場所＝1\n","    return result**2\n","\n","def get_lof(f_model, x):\n","    train = f_model.predict(x)\n","    train = train.reshape((len(train),-1))\n","\n","    ms = MinMaxScaler()\n","    train = ms.fit_transform(train)\n","\n","    # fit the model\n","    lof = LocalOutlierFactor(n_neighbors=5)\n","    y_pred = lof.fit(train[:1000])\n","\n","    return lof, ms\n","\n","def connect(x, x_predict):\n","    jet = cv2.applyColorMap(np.uint8(255 * x_predict), cv2.COLORMAP_JET)  # 疑似的に色をつける\n","    jet = cv2.cvtColor(jet, cv2.COLOR_BGR2RGB)  # 色をRGBに変換\n","    jet = (np.float32(jet) + x*255 / 2)   # もとの画像に合成\n","    return array_to_img(jet)\n","\n","def heatmap(target, lof, ms, p_model, f_model):\n","    map1 = grid_score(target, lof, ms, p_model, f_model, divide=20)\n","    map2 = grid_score(target, lof, ms, p_model, f_model, divide=18)\n","    map3 = grid_score(target, lof, ms, p_model, f_model, divide=16)\n","    map4 = grid_score(target, lof, ms, p_model, f_model, divide=14)\n","    result = map1 + map2 + map3 + map4\n","\n","    plt.figure(figsize=(8,16))\n","    plt.subplot(1,2,1)\n","    plt.imshow(target)\n","    plt.axis(\"off\")\n","\n","    plt.subplot(1,2,2)\n","    plt.imshow(connect(target, result/np.max(result)))\n","    plt.axis(\"off\")\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"udsWAFa3PTB0","colab_type":"code","outputId":"63fdee35-f209-41ab-ee4b-ab54069a2033","executionInfo":{"status":"ok","timestamp":1569303409213,"user_tz":-540,"elapsed":1217963,"user":{"displayName":"Rubu Kuma","photoUrl":"","userId":"02985854947085672136"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-CsvdjvxjgZ_FLnKmovW5A9pclGnbgQJ"}},"source":["lof, ms = get_lof(f_model, x_train)\n","\n","for i in range(20):\n","    print(i)\n","    target = np.copy(x_test_anomaly_256[i])\n","    heatmap(target, lof, ms, p_model, f_model)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}